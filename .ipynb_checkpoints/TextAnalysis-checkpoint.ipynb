{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "Let's perform some basic text analysis tasks such as accessing data, tokenizing a corpus, and computing token frequencies, on a Wikipedia article and on the NLTK Reuters corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "2ae2f44bae076d61ac4a256a635ee44d",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import requests\n",
    "import json\n",
    "import string\n",
    "\n",
    "import html2text\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [Wikipedia article on Text Mining](https://en.wikipedia.org/wiki/Text_mining) as a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "bef40ff651267457a5501204250c604f",
     "grade": false,
     "grade_id": "syllabus_text",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Text mining\n",
      "\n",
      "From Wikipedia, the free encyclopedia\n",
      "\n",
      "Jump to: navigation, search\n",
      "\n",
      "**Text mining**, also referred to as _text data mining_, roughly equivalent to **text analytics**, refers to the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (_i.e._, learning relations between named entities).\n",
      "\n",
      "Text analysis involves information retrieval, lexical analysis to study word\n",
      "frequency distributions, pattern recognition, tagging/annotation, information\n",
      "extraction, data mining techniques including link and association analysis,\n",
      "visualization, and predictive analytics. The overarching goal is, essentially,\n",
      "to turn text into data for analysis, via application of natural language\n",
      "processing (NLP) and analytical methods.\n",
      "\n",
      "A typical application is to scan a set of documents written in a natural\n",
      "language and either model the document set for predictive classification\n",
      "purposes or populate a database or search index with the information\n",
      "extracted.\n",
      "\n",
      "## Contents\n",
      "\n",
      "  * 1 Text mining and text analytics\n",
      "  * 2 History\n",
      "  * 3 Text analysis processes\n",
      "  * 4 Applications\n",
      "    * 4.1 Security applications\n",
      "    * 4.2 Biomedical applications\n",
      "    * 4.3 Software applications\n",
      "    * 4.4 Online media applications\n",
      "    * 4.5 Marketing applications\n",
      "    * 4.6 Sentiment analysis\n",
      "    * 4.7 Academic applications\n",
      "    * 4.8 Digital Humanities and Computational Sociology\n",
      "  * 5 Software\n",
      "  * 6 Intellectual Property Law and Text Mining\n",
      "    * 6.1 Situation in Europe\n",
      "    * 6.2 Situation in United States\n",
      "  * 7 Implications\n",
      "  * 8 See also\n",
      "  * 9 Notes\n",
      "  * 10 References\n",
      "  * 11 External links\n",
      "\n",
      "## Text mining and text analytics[edit]\n",
      "\n",
      "The term **text analytics** describes a set of linguistic, statistical, and\n",
      "machine learning techniques that model and structure the information content\n",
      "of textual sources for business intelligence, exploratory data analysis,\n",
      "research, or investigation.[1] The term is roughly synonymous with text\n",
      "mining; indeed, Ronen Feldman modified a 2000 description of \"text mining\"[2]\n",
      "in 2004 to describe \"text analytics.\"[3] The latter term is now used more\n",
      "frequently in business settings while \"text mining\" is used in some of the\n",
      "earliest application areas, dating to the 1980s,[4] notably life-sciences\n",
      "research and government intelligence.\n",
      "\n",
      "The term text analytics also describes that application of text analytics to\n",
      "respond to business problems, whether independently or in conjunction with\n",
      "query and analysis of fielded, numerical data. It is a truism that 80 percent\n",
      "of business-relevant information originates in unstructured form, primarily\n",
      "text.[5] These techniques and processes discover and present knowledge –\n",
      "facts, business rules, and relationships – that is otherwise locked in textual\n",
      "form, impenetrable to automated processing.\n",
      "\n",
      "## History[edit]\n",
      "\n",
      "Labor-intensive manual text mining approaches first surfaced in the\n",
      "mid-1980s,[6] but technological advances have enabled the field to advance\n",
      "during the past decade. Text mining is an interdisciplinary field that draws\n",
      "on information retrieval, data mining, machine learning, statistics, and\n",
      "computational linguistics. As most information (common estimates say over\n",
      "80%)[5] is currently stored as text, text mining is believed to have a high\n",
      "commercial potential value. Increasing interest is being paid to multilingual\n",
      "data mining: the ability to gain information across languages and cluster\n",
      "similar items from different linguistic sources according to their meaning.\n",
      "\n",
      "The challenge of exploiting the large proportion of enterprise information\n",
      "that originates in \"unstructured\" form has been recognized for decades.[7] It\n",
      "is recognized in the earliest definition of business intelligence (BI), in an\n",
      "October 1958 IBM Journal article by H.P. Luhn, A Business Intelligence System,\n",
      "which describes a system that will:\n",
      "\n",
      "> \"...utilize data-processing machines for auto-abstracting and auto-encoding\n",
      "of documents and for creating interest profiles for each of the 'action\n",
      "points' in an organization. Both incoming and internally generated documents\n",
      "are automatically abstracted, characterized by a word pattern, and sent\n",
      "automatically to appropriate action points.\"\n",
      "\n",
      "Yet as management information systems developed starting in the 1960s, and as\n",
      "BI emerged in the '80s and '90s as a software category and field of practice,\n",
      "the emphasis was on numerical data stored in relational databases. This is not\n",
      "surprising: text in \"unstructured\" documents is hard to process. The emergence\n",
      "of text analytics in its current form stems from a refocusing of research in\n",
      "the late 1990s from algorithm development to application, as described by\n",
      "Prof. Marti A. Hearst in the paper Untangling Text Data Mining:[8]\n",
      "\n",
      "> For almost a decade the computational linguistics community has viewed large\n",
      "text collections as a resource to be tapped in order to produce better text\n",
      "analysis algorithms. In this paper, I have attempted to suggest a new\n",
      "emphasis: the use of large online text collections to discover new facts and\n",
      "trends about the world itself. I suggest that to make progress we do not need\n",
      "fully artificial intelligent text analysis; rather, a mixture of\n",
      "computationally-driven and user-guided analysis may open the door to exciting\n",
      "new results.\n",
      "\n",
      "Hearst's 1999 statement of need fairly well describes the state of text\n",
      "analytics technology and practice a decade later.\n",
      "\n",
      "## Text analysis processes[edit]\n",
      "\n",
      "Subtasks — components of a larger text-analytics effort — typically include:\n",
      "\n",
      "  * Information retrieval or identification of a corpus is a preparatory step: collecting or identifying a set of textual materials, on the Web or held in a file system, database, or content corpus manager, for analysis.\n",
      "  * Although some text analytics systems apply exclusively advanced statistical methods, many others apply more extensive natural language processing, such as part of speech tagging, syntactic parsing, and other types of linguistic analysis.[_citation needed_]\n",
      "  * Named entity recognition is the use of gazetteers or statistical techniques to identify named text features: people, organizations, place names, stock ticker symbols, certain abbreviations, and so on. Disambiguation — the use of contextual clues — may be required to decide where, for instance, \"Ford\" can refer to a former U.S. president, a vehicle manufacturer, a movie star, a river crossing, or some other entity.\n",
      "  * Recognition of Pattern Identified Entities: Features such as telephone numbers, e-mail addresses, quantities (with units) can be discerned via regular expression or other pattern matches.\n",
      "  * Coreference: identification of noun phrases and other terms that refer to the same object.\n",
      "  * Relationship, fact, and event Extraction: identification of associations among entities and other information in text\n",
      "  * Sentiment analysis involves discerning subjective (as opposed to factual) material and extracting various forms of attitudinal information: sentiment, opinion, mood, and emotion. Text analytics techniques are helpful in analyzing, sentiment at the entity, concept, or topic level and in distinguishing opinion holder and opinion object.[9]\n",
      "  * Quantitative text analysis is a set of techniques stemming from the social sciences where either a human judge or a computer extracts semantic or grammatical relationships between words in order to find out the meaning or stylistic patterns of, usually, a casual personal text for the purpose of psychological profiling etc.[10]\n",
      "\n",
      "## Applications[edit]\n",
      "\n",
      "The technology is now broadly applied for a wide variety of government,\n",
      "research, and business needs. Applications can be sorted into a number of\n",
      "categories by analysis type or by business function. Using this approach to\n",
      "classifying solutions, application categories include:\n",
      "\n",
      "  * Enterprise Business Intelligence/Data Mining, Competitive Intelligence\n",
      "  * E-Discovery, Records Management\n",
      "  * National Security/Intelligence\n",
      "  * Scientific discovery, especially Life Sciences\n",
      "  * Sentiment Analysis Tools, Listening Platforms\n",
      "  * Natural Language/Semantic Toolkit or Service\n",
      "  * Publishing\n",
      "  * Automated ad placement\n",
      "  * Search/Information Access\n",
      "  * Social media monitoring\n",
      "\n",
      "### Security applications[edit]\n",
      "\n",
      "Many text mining software packages are marketed for security applications,\n",
      "especially monitoring and analysis of online plain text sources such as\n",
      "Internet news, blogs, etc. for national security purposes.[11] It is also\n",
      "involved in the study of text encryption/decryption.\n",
      "\n",
      "### Biomedical applications[edit]\n",
      "\n",
      "Main article: Biomedical text mining\n",
      "\n",
      "A range of text mining applications in the biomedical literature has been\n",
      "described.[12]\n",
      "\n",
      "One online text mining application in the biomedical literature is PubGene\n",
      "that combines biomedical text mining with network visualization as an Internet\n",
      "service.[13][14] TPX is a concept-assisted search and navigation tool for\n",
      "biomedical literature analyses[15] \\- it runs on PubMed/PMC and can be\n",
      "configured, on request, to run on local literature repositories too.\n",
      "\n",
      "GoPubMed is a knowledge-based search engine for biomedical texts.\n",
      "\n",
      "### Software applications[edit]\n",
      "\n",
      "Text mining methods and software is also being researched and developed by\n",
      "major firms, including IBM and Microsoft, to further automate the mining and\n",
      "analysis processes, and by different firms working in the area of search and\n",
      "indexing in general as a way to improve their results. Within public sector\n",
      "much effort has been concentrated on creating software for tracking and\n",
      "monitoring terrorist activities.[16]\n",
      "\n",
      "### Online media applications[edit]\n",
      "\n",
      "Text mining is being used by large media companies, such as the Tribune\n",
      "Company, to clarify information and to provide readers with greater search\n",
      "experiences, which in turn increases site \"stickiness\" and revenue.\n",
      "Additionally, on the back end, editors are benefiting by being able to share,\n",
      "associate and package news across properties, significantly increasing\n",
      "opportunities to monetize content.\n",
      "\n",
      "### Marketing applications[edit]\n",
      "\n",
      "Text mining is starting to be used in marketing as well, more specifically in\n",
      "analytical customer relationship management.[17] Coussement and Van den Poel\n",
      "(2008)[18][19] apply it to improve predictive analytics models for customer\n",
      "churn (customer attrition).[18]\n",
      "\n",
      "### Sentiment analysis[edit]\n",
      "\n",
      "Sentiment analysis may involve analysis of movie reviews for estimating how\n",
      "favorable a review is for a movie.[20] Such an analysis may need a labeled\n",
      "data set or labeling of the affectivity of words. Resources for affectivity of\n",
      "words and concepts have been made for WordNet[21] and ConceptNet,[22]\n",
      "respectively.\n",
      "\n",
      "Text has been used to detect emotions in the related area of affective\n",
      "computing.[23] Text based approaches to affective computing have been used on\n",
      "multiple corpora such as students evaluations, children stories and news\n",
      "stories.\n",
      "\n",
      "### Academic applications[edit]\n",
      "\n",
      "The issue of text mining is of importance to publishers who hold large\n",
      "databases of information needing indexing for retrieval. This is especially\n",
      "true in scientific disciplines, in which highly specific information is often\n",
      "contained within written text. Therefore, initiatives have been taken such as\n",
      "Nature's proposal for an Open Text Mining Interface (OTMI) and the National\n",
      "Institutes of Health's common Journal Publishing Document Type Definition\n",
      "(DTD) that would provide semantic cues to machines to answer specific queries\n",
      "contained within text without removing publisher barriers to public access.\n",
      "\n",
      "Academic institutions have also become involved in the text mining initiative:\n",
      "\n",
      "  * The National Centre for Text Mining (NaCTeM), is the first publicly funded text mining centre in the world. NaCTeM is operated by the University of Manchester[24] in close collaboration with the Tsujii Lab,[25] University of Tokyo.[26] NaCTeM provides customised tools, research facilities and offers advice to the academic community. They are funded by the Joint Information Systems Committee (JISC) and two of the UK Research Councils (EPSRC &amp; BBSRC). With an initial focus on text mining in the biological and biomedical sciences, research has since expanded into the areas of social sciences.\n",
      "  * In the United States, the School of Information at University of California, Berkeley is developing a program called BioText to assist biology researchers in text mining and analysis.\n",
      "\n",
      "### Digital Humanities and Computational Sociology[edit]\n",
      "\n",
      "The automatic analysis of vast textual corpora has created the possibility for\n",
      "scholars to analyse millions of documents in multiple languages with very\n",
      "limited manual intervention. Key enabling technologies have been Parsing,\n",
      "Machine Translation, Topic categorization, Machine Learning.\n",
      "\n",
      "![](//upload.wikimedia.org/wikipedia/commons/thumb/4/43/Tripletsnew2012.png/220px-\n",
      "Tripletsnew2012.png)\n",
      "\n",
      "Narrative network of US Elections 2012[27]\n",
      "\n",
      "The automatic parsing of textual corpora has enabled the extraction of actors\n",
      "and their relational networks on a vast scale, turning textual data into\n",
      "network data. The resulting networks, which can contain thousands of nodes,\n",
      "are then analysed by using tools from Network theory to identify the key\n",
      "actors, the key communities or parties, and general properties such as\n",
      "robustness or structural stability of the overall network, or centrality of\n",
      "certain nodes.[28] This automates the approach introduced by Quantitative\n",
      "Narrative Analysis,[29] whereby subject-verb-object triplets are identified\n",
      "with pairs of actors linked by an action, or pairs formed by actor-object.[27]\n",
      "\n",
      "Content analysis has been a traditional part of social sciences and media\n",
      "studies for a long time. The automation of content analysis has allowed a \"big\n",
      "data\" revolution to take place in that field, with studies in social media and\n",
      "newspaper content that include millions of news items. Gender bias,\n",
      "readability, content similarity, reader preferences, and even mood have been\n",
      "analyzed based on text mining methods over millions of documents. [30] [31]\n",
      "[32] [33] The analysis of readability, gender bias and topic bias was\n",
      "demonstrated in [34] showing how different topics have different gender biases\n",
      "and levels of readability; the possibility to detect mood shifts in a vast\n",
      "population by analysing Twitter content was demonstrated as well.[35]\n",
      "\n",
      "## Software[edit]\n",
      "\n",
      "Text mining computer programs are available from many commercial and open\n",
      "source companies and sources. See List of text mining software.\n",
      "\n",
      "## Intellectual Property Law and Text Mining[edit]\n",
      "\n",
      "### Situation in Europe[edit]\n",
      "\n",
      "Due to a lack of flexibilities in European copyright and database law, the\n",
      "mining of in-copyright works such as web mining without the permission of the\n",
      "copyright owner is not legal. In the UK in 2014, on the recommendation of the\n",
      "Hargreaves review the government amended copyright law[36] to allow text\n",
      "mining as a limitation and exception. Only the second country in the world to\n",
      "do so after Japan, which introduced a mining specific exception in 2009.\n",
      "However, due to the restriction of the Copyright Directive, the UK exception\n",
      "only allows content mining for non-commercial purposes. UK copyright law does\n",
      "not allow this provision to be overridden by contractual terms and conditions.\n",
      "\n",
      "The European Commission facilitated stakeholder discussion on text and data\n",
      "mining in 2013, under the title of Licences for Europe.[37] The focus on the\n",
      "solution to this legal issue being licences and not limitations and exceptions\n",
      "to copyright law led to representatives of universities, researchers,\n",
      "libraries, civil society groups and open access publishers to leave the\n",
      "stakeholder dialogue in May 2013.[38]\n",
      "\n",
      "### Situation in United States[edit]\n",
      "\n",
      "By contrast to Europe, the flexible nature of US copyright law, and in\n",
      "particular fair use means that text mining in America, as well as other fair\n",
      "use countries such as Israel, Taiwan and South Korea is viewed as being legal.\n",
      "As text mining is transformative, meaning that it does not supplant the\n",
      "original work, it is viewed as being lawful under fair use. For example, as\n",
      "part of the Google Book settlement the presiding judge on the case ruled that\n",
      "Google's digitisation project of in-copyright books was lawful, in part\n",
      "because of the transformative uses that the digitisation project displayed -\n",
      "one such use being text and data mining.[39]\n",
      "\n",
      "## Implications[edit]\n",
      "\n",
      "Until recently, websites most often used text-based searches, which only found\n",
      "documents containing specific user-defined words or phrases. Now, through use\n",
      "of a semantic web, text mining can find content based on meaning and context\n",
      "(rather than just by a specific word). Additionally, text mining software can\n",
      "be used to build large dossiers of information about specific people and\n",
      "events. For example, large datasets based on data extracted from news reports\n",
      "can be built to facilitate social networks analysis or counter-intelligence.\n",
      "In effect, the text mining software may act in a capacity similar to an\n",
      "intelligence analyst or research librarian, albeit with a more limited scope\n",
      "of analysis. Text mining is also used in some email spam filters as a way of\n",
      "determining the characteristics of messages that are likely to be\n",
      "advertisements or other unwanted material. Text mining plays an important role\n",
      "in determining financial market sentiment.\n",
      "\n",
      "## See also[edit]\n",
      "\n",
      "  * Full text search\n",
      "  * Concept Mining\n",
      "  * Web mining, a task that may involve text mining (e.g. first find appropriate web pages by classifying crawled web pages, then extract the desired information from the text content of these pages considered relevant).\n",
      "  * Sequential pattern mining: String and Sequence Mining\n",
      "  * News analytics\n",
      "  * Market sentiment\n",
      "  * Named entity recognition\n",
      "  * Name resolution (semantics and text extraction)\n",
      "  * Record linkage\n",
      "  * w-shingling\n",
      "  * List of text mining software\n",
      "\n",
      "## Notes[edit]\n",
      "\n",
      "  1. **^** [1] Archived November 29, 2009, at the Wayback Machine.\n",
      "  2. **^** \"KDD-2000 Workshop on Text Mining - Call for Papers\". Cs.cmu.edu. Retrieved 2015-02-23.\n",
      "  3. **^** [2] Archived March 3, 2012, at the Wayback Machine.\n",
      "  4. **^** Hobbs, Jerry R.; Walker, Donald E.; Amsler, Robert A. (1982). \"Proceedings of the 9th conference on Computational linguistics\" **1**: 127–32. doi:10.3115/991813.991833.\n",
      "  5. ^ _**a**_ _**b**_ \"Unstructured Data and the 80 Percent Rule\". Breakthrough Analysis. Retrieved 2015-02-23.\n",
      "  6. **^** \"Content Analysis of Verbatim Explanations\". Ppc.sas.upenn.edu. Retrieved 2015-02-23.\n",
      "  7. **^** \"A Brief History of Text Analytics by Seth Grimes\". Beyenetwork. 2007-10-30. Retrieved 2015-02-23.\n",
      "  8. **^** Hearst, Marti A. (1999). \"Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics\": 3–10. doi:10.3115/1034678.1034679. ISBN 1-55860-609-2.\n",
      "  9. **^** \"Full Circle Sentiment Analysis\". Breakthrough Analysis. Retrieved 2015-02-23.\n",
      "  10. **^** Mehl, Matthias R. (2006). \"Handbook of multimethod measurement in psychology\": 141. doi:10.1037/11383-011. ISBN 1-59147-318-7.\n",
      "  11. **^** Zanasi, Alessandro (2009). \"Proceedings of the International Workshop on Computational Intelligence in Security for Information Systems CISIS'08\". Advances in Soft Computing **53**: 53. doi:10.1007/978-3-540-88181-0_7. ISBN 978-3-540-88180-3.\n",
      "  12. **^** Cohen, K. Bretonnel; Hunter, Lawrence (2008). \"Getting Started in Text Mining\". _PLoS Computational Biology_ **4** (1): e20. doi:10.1371/journal.pcbi.0040020. PMC 2217579. PMID 18225946. ![open access publication - free to read](//upload.wikimedia.org/wikipedia/commons/thumb/7/77/Open_Access_logo_PLoS_transparent.svg/9px-Open_Access_logo_PLoS_transparent.svg.png)\n",
      "  13. **^** Jenssen, Tor-Kristian; Lægreid, Astrid; Komorowski, Jan; Hovig, Eivind (2001). \"A literature network of human genes for high-throughput analysis of gene expression\". _Nature Genetics_ **28** (1): 21–8. doi:10.1038/ng0501-21. PMID 11326270.\n",
      "  14. **^** Masys, Daniel R. (2001). \"Linking microarray data to the literature\". _Nature Genetics_ **28** (1): 9–10. doi:10.1038/ng0501-9. PMID 11326264.\n",
      "  15. **^** Joseph, Thomas; Saipradeep, Vangala G; Venkat Raghavan, Ganesh Sekar; Srinivasan, Rajgopal; Rao, Aditya; Kotte, Sujatha; Sivadasan, Naveen (2012). \"TPX: Biomedical literature search made easy\". _Bioinformation_ **8** (12): 578–80. doi:10.6026/97320630008578. PMC 3398782. PMID 22829734.\n",
      "  16. **^** [3] Archived October 4, 2013, at the Wayback Machine.\n",
      "  17. **^** \"Text Analytics\". Medallia. Retrieved 2015-02-23.\n",
      "  18. ^ _**a**_ _**b**_ Coussement, Kristof; Van Den Poel, Dirk (2008). \"Integrating the voice of customers through call center emails into a decision support system for churn prediction\". _Information &amp; Management_ **45** (3): 164–74. doi:10.1016/j.im.2008.01.005.\n",
      "  19. **^** Coussement, Kristof; Van Den Poel, Dirk (2008). \"Improving customer complaint management by automatic email classification using linguistic style features as predictors\". _Decision Support Systems_ **44** (4): 870–82. doi:10.1016/j.dss.2007.10.010.\n",
      "  20. **^** Pang, Bo; Lee, Lillian; Vaithyanathan, Shivakumar (2002). \"Proceedings of the ACL-02 conference on Empirical methods in natural language processing\" **10**: 79–86. doi:10.3115/1118693.1118704.\n",
      "  21. **^** Alessandro Valitutti; Carlo Strapparava; Oliviero Stock (2005). \"Developing Affective Lexical Resources\" (PDF). _Psychology Journal_ **2** (1): 61–83.\n",
      "  22. **^** Erik Cambria; Robert Speer; Catherine Havasi; Amir Hussain (2010). \"SenticNet: a Publicly Available Semantic Resource for Opinion Mining\" (PDF). _Proceedings of AAAI CSK_. pp. 14–18.\n",
      "  23. **^** Calvo, Rafael A; d'Mello, Sidney (2010). \"Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications\". _IEEE Transactions on Affective Computing_ **1** (1): 18–37. doi:10.1109/T-AFFC.2010.1.\n",
      "  24. **^** \"The University of Manchester\". Manchester.ac.uk. Retrieved 2015-02-23.\n",
      "  25. **^** \"Tsujii Laboratory\". Tsujii.is.s.u-tokyo.ac.jp. Retrieved 2015-02-23.\n",
      "  26. **^** \"The University of Tokyo\". UTokyo. Retrieved 2015-02-23.\n",
      "  27. ^ _**a**_ _**b**_ Automated analysis of the US presidential elections using Big Data and network analysis; S Sudhahar, GA Veltri, N Cristianini; Big Data &amp; Society 2 (1), 1-28, 2015\n",
      "  28. **^** Network analysis of narrative content in large corpora; S Sudhahar, G De Fazio, R Franzosi, N Cristianini; Natural Language Engineering, 1-32, 2013\n",
      "  29. **^** Quantitative Narrative Analysis; Roberto Franzosi; Emory University © 2010\n",
      "  30. **^** I. Flaounas, M. Turchi, O. Ali, N. Fyson, T. De Bie, N. Mosdell, J. Lewis, N. Cristianini, The Structure of EU Mediasphere, PLoS ONE, Vol. 5(12), pp. e14243, 2010.\n",
      "  31. **^** Nowcasting Events from the Social Web with Statistical Learning V Lampos, N Cristianini; ACM Transactions on Intelligent Systems and Technology (TIST) 3 (4), 72\n",
      "  32. **^** NOAM: news outlets analysis and monitoring system; I Flaounas, O Ali, M Turchi, T Snowsill, F Nicart, T De Bie, N Cristianini Proc. of the 2011 ACM SIGMOD international conference on Management of data\n",
      "  33. **^** Automatic discovery of patterns in media content, N Cristianini, Combinatorial Pattern Matching, 2-13, 2011\n",
      "  34. **^** I. Flaounas, O. Ali, T. Lansdall-Welfare, T. De Bie, N. Mosdell, J. Lewis, N. Cristianini, RESEARCH METHODS IN THE AGE OF DIGITAL JOURNALISM, Digital Journalism, Routledge, 2012\n",
      "  35. **^** Effects of the Recession on Public Mood in the UK; T Lansdall-Welfare, V Lampos, N Cristianini; Mining Social Network Dynamics (MSND) session on Social Media Applications\n",
      "  36. **^** [4] Archived June 9, 2014, at the Wayback Machine.\n",
      "  37. **^** \"Licences for Europe - Structured Stakeholder Dialogue 2013\". _European Commission_. Retrieved 14 November 2014.\n",
      "  38. **^** \"Text and Data Mining:Its importance and the need for change in Europe\". _Association of European Research Libraries_. Retrieved 14 November 2014.\n",
      "  39. **^** \"Judge grants summary judgment in favor of Google Books — a fair use victory\". _Lexology.com_. Antonelli Law Ltd. Retrieved 14 November 2014.\n",
      "\n",
      "## References[edit]\n",
      "\n",
      "  * Ananiadou, S. and McNaught, J. (Editors) (2006). _Text Mining for Biology and Biomedicine_. Artech House Books. ISBN 978-1-58053-984-5\n",
      "  * Bilisoly, R. (2008). _Practical Text Mining with Perl_. New York: John Wiley &amp; Sons. ISBN 978-0-470-17643-6\n",
      "  * Feldman, R., and Sanger, J. (2006). _The Text Mining Handbook_. New York: Cambridge University Press. ISBN 978-0-521-83657-9\n",
      "  * Indurkhya, N., and Damerau, F. (2010). _Handbook Of Natural Language Processing_, 2nd Edition. Boca Raton, FL: CRC Press. ISBN 978-1-4200-8592-1\n",
      "  * Kao, A., and Poteet, S. (Editors). _Natural Language Processing and Text Mining_. Springer. ISBN 1-84628-175-X\n",
      "  * Konchady, M. _Text Mining Application Programming (Programming Series)_. Charles River Media. ISBN 1-58450-460-9\n",
      "  * Manning, C., and Schutze, H. (1999). _Foundations of Statistical Natural Language Processing_. Cambridge, MA: MIT Press. ISBN 978-0-262-13360-9\n",
      "  * Miner, G., Elder, J., Hill. T, Nisbet, R., Delen, D. and Fast, A. (2012). _Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications_. Elsevier Academic Press. ISBN 978-0-12-386979-1\n",
      "  * McKnight, W. (2005). \"Building business intelligence: Text data mining in business intelligence\". _DM Review_, 21-22.\n",
      "  * Srivastava, A., and Sahami. M. (2009). _Text Mining: Classification, Clustering, and Applications_. Boca Raton, FL: CRC Press. ISBN 978-1-4200-5940-3\n",
      "  * Zanasi, A. (Editor) (2007). _Text Mining and its Applications to Intelligence, CRM and Knowledge Management_. WIT Press. ISBN 978-1-84564-131-3\n",
      "\n",
      "## External links[edit]\n",
      "\n",
      "  * Marti Hearst: What Is Text Mining? (October, 2003)\n",
      "  * Automatic Content Extraction, Linguistic Data Consortium\n",
      "  * Automatic Content Extraction, NIST\n",
      "  * Research work and applications of Text Mining (for instance AgroNLP)\n",
      "  * Text Mining Free Tools\n",
      "\n",
      "Authority control |\n",
      "\n",
      "  * NDL: 01119322\n",
      "\n",
      "  \n",
      "  \n",
      "---|---  \n",
      "  \n",
      "![](//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1)\n",
      "\n",
      "Retrieved from\n",
      "\"https://en.wikipedia.org/w/index.php?title=Text_mining&amp;oldid=723929281\"\n",
      "\n",
      "Categories:\n",
      "\n",
      "  * Artificial intelligence applications\n",
      "  * Data mining\n",
      "  * Computational linguistics\n",
      "  * Data analysis\n",
      "  * Natural language processing\n",
      "  * Statistical natural language processing\n",
      "\n",
      "Hidden categories:\n",
      "\n",
      "  * All articles with unsourced statements\n",
      "  * Articles with unsourced statements from February 2012\n",
      "\n",
      "## Navigation menu\n",
      "\n",
      "### Personal tools\n",
      "\n",
      "  * Not logged in\n",
      "  * Talk\n",
      "  * Contributions\n",
      "  * Create account\n",
      "  * Log in\n",
      "\n",
      "### Namespaces\n",
      "\n",
      "  * Article\n",
      "  * Talk\n",
      "\n",
      "###  Variants\n",
      "\n",
      "### Views\n",
      "\n",
      "  * Read\n",
      "  * Edit\n",
      "  * View history\n",
      "\n",
      "### More\n",
      "\n",
      "###  Search\n",
      "\n",
      "### Navigation\n",
      "\n",
      "  * Main page\n",
      "  * Contents\n",
      "  * Featured content\n",
      "  * Current events\n",
      "  * Random article\n",
      "  * Donate to Wikipedia\n",
      "  * Wikipedia store\n",
      "\n",
      "### Interaction\n",
      "\n",
      "  * Help\n",
      "  * About Wikipedia\n",
      "  * Community portal\n",
      "  * Recent changes\n",
      "  * Contact page\n",
      "\n",
      "### Tools\n",
      "\n",
      "  * What links here\n",
      "  * Related changes\n",
      "  * Upload file\n",
      "  * Special pages\n",
      "  * Permanent link\n",
      "  * Page information\n",
      "  * Wikidata item\n",
      "  * Cite this page\n",
      "\n",
      "### Print/export\n",
      "\n",
      "  * Create a book\n",
      "  * Download as PDF\n",
      "  * Printable version\n",
      "\n",
      "### In other projects\n",
      "\n",
      "  * Wikimedia Commons\n",
      "\n",
      "### Languages\n",
      "\n",
      "  * العربية\n",
      "  * Čeština\n",
      "  * Deutsch\n",
      "  * Español\n",
      "  * فارسی\n",
      "  * Français\n",
      "  * Bahasa Indonesia\n",
      "  * Magyar\n",
      "  * Nederlands\n",
      "  * 日本語\n",
      "  * Polski\n",
      "  * Português\n",
      "  * Русский\n",
      "  * Svenska\n",
      "  * ไทย\n",
      "  * Türkçe\n",
      "  * Українська\n",
      "  * Tiếng Việt\n",
      "  * 中文\n",
      "  * \n",
      "\n",
      "Edit links\n",
      "\n",
      "  * This page was last modified on 6 June 2016, at 03:27.\n",
      "  * Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n",
      "\n",
      "  * Privacy policy\n",
      "  * About Wikipedia\n",
      "  * Disclaimers\n",
      "  * Contact Wikipedia\n",
      "  * Developers\n",
      "  * Cookie statement\n",
      "  * Mobile view\n",
      "\n",
      "  * ![Wikimedia Foundation](/static/images/wikimedia-button.png)\n",
      "  * ![Powered by MediaWiki](/static/images/poweredby_mediawiki_88x31.png)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h = html2text.HTML2Text()\n",
    "h.ignore_links=True\n",
    "\n",
    "resp = requests.get('https://en.wikipedia.org/wiki/Text_mining')\n",
    "text = h.handle(resp.text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "- Tokenize the text string `syllabus_text`.\n",
    "  You should clean up the list of tokens by removing all puntuation tokens\n",
    "  and keeping only tokens with one or more alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "64d6bd63cc50a60e576ea203b59dca52",
     "grade": false,
     "grade_id": "tokenize_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'mining', 'from', 'wikipedia', 'the'] ... ['mediawiki', 'static', 'images', 'poweredby_mediawiki_88x31', 'png']\n"
     ]
    }
   ],
   "source": [
    "syllabus_words = get_words(text)\n",
    "print(syllabus_words[:5], '...', syllabus_words[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Diversity\n",
    "\n",
    "- Function `count()` computes the the number of tokens, number of words, and lexical diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "d0a1c5dd1889769c11a5ebcb19376384",
     "grade": false,
     "grade_id": "count_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article has 1483 tokens and 4178 words for a lexical diversity of 2.817\n"
     ]
    }
   ],
   "source": [
    "num_tokens, num_words, lex_div = count(syllabus_words)\n",
    "print(\"Article has {0} tokens and {1} words for a lexical diversity of {2:4.3f}\"\n",
    "      \"\".format(num_tokens, num_words, lex_div))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common occurrences\n",
    "\n",
    "- Function `get_most_common()` computes the most commonly occurring terms and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "94cf04986e6ecc93f648aa481d43a51f",
     "grade": false,
     "grade_id": "get_most_common_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term        : Count\n",
      "--------------------\n",
      "the         :  136\n",
      "of          :  128\n",
      "and         :  113\n",
      "text        :  102\n",
      "in          :   84\n",
      "mining      :   76\n",
      "a           :   75\n",
      "to          :   74\n",
      "analysis    :   45\n",
      "for         :   42\n"
     ]
    }
   ],
   "source": [
    "syllabus_most_common = get_most_common(syllabus_words, 10)\n",
    "\n",
    "print('{0:12s}: {1}'.format('Term', 'Count'))\n",
    "print(20*'-')\n",
    "\n",
    "for token, freq in syllabus_most_common:\n",
    "    print('{0:12s}: {1:4d}'.format(token, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hapax\n",
    "\n",
    "- Function `find_hapaxes()` finds all hapexes in a text string.\n",
    "- A hapax is a word that exists only once. Since we are looking at Wikipedia, we expect difference languages to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "aa6278b0d73f6e918c8d532b4740819a",
     "grade": false,
     "grade_id": "find_hapaxes_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yet', 'you', 'čeština', 'русский', 'українська', 'العربية', 'فارسی', 'ไทย', '中文', '日本語']\n"
     ]
    }
   ],
   "source": [
    "syllabus_hapaxes = find_hapaxes(syllabus_words)\n",
    "print(sorted(syllabus_hapaxes)[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK corpus\n",
    "\n",
    "Let's take a look at the NLTK Reuters corpus. See the [NLTK docs](http://www.nltk.org/book/ch02.html#reuters-corpus) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "2cd30c8732682eaf6565fb9504a2ee3a",
     "grade": false,
     "grade_id": "import_reuters",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical diversity in corpus\n",
    "\n",
    "- Function `count_corpus()` computes the the number of token, number of words, and lexical diversity.\n",
    "- Uses the `words()` function of the reuters object, which includes non-alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "13d5ec1770367e7efc73b80ec73df38a",
     "grade": false,
     "grade_id": "count_corpus_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Reuters corpus has 41600 tokens and 1720901 words for a lexical diversity of 41.368\n"
     ]
    }
   ],
   "source": [
    "num_words, num_tokens, lex_div = count_corpus(reuters)\n",
    "print(\"The Reuters corpus has {0} tokens and {1} words for a lexical diversity of {2:4.3f}\"\n",
    "      \"\".format(num_tokens, num_words, lex_div))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long words\n",
    "\n",
    "- Function `get_long_words()` searches for all words in corpus that are longer than a specific length of characters\n",
    "- We will pass in a length of 20 to find all worders longer than 20 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "7f8b9a9c147d66772deada4a0cafa25a",
     "grade": false,
     "grade_id": "get_long_words_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['discontinuedoperations', 'Beteiligungsgesellschaft', 'Gloeielampenfabrieken', '..........................................', 'Warenhandelsgesellschaft']\n"
     ]
    }
   ],
   "source": [
    "long_words = get_long_words(reuters, length=20)\n",
    "print(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "Next, we can perform text classification tasks by using the nltk and the scikit learn machine learning libraries. We will continue to examine the Reuters corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories\n",
    "\n",
    "Before delving into text data mining, let's first explore the Reuters data set.\n",
    "\n",
    "- Function `get_categories()`, given an NLTK corpus object, returns its categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "bd83d7db1f8d51b19c2844705cde5ef0",
     "grade": false,
     "grade_id": "get_categories_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "all_categories = get_categories(reuters)\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fileids\n",
    "\n",
    "- Function `get_fileids()` finds all `fileids` of a corpus, in our case Reuters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "85c2c97ebd68e8cc4fdac96b236941ad",
     "grade": false,
     "grade_id": "get_fileids_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test/14826', 'test/14828', 'test/14829', 'test/14832', 'test/14833'] ... ['training/999', 'training/9992', 'training/9993', 'training/9994', 'training/9995']\n"
     ]
    }
   ],
   "source": [
    "fileids = get_fileids(reuters)\n",
    "print(fileids[:5], '...', fileids[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of categories\n",
    "\n",
    "The corpus contains 10,788 documents (`fileids`) which have been classified into 90 topics (`categories`). We will use those 10,788 documents to train machine learning models and try to predict which topic each document belongs to.\n",
    "\n",
    "Our next function will find categories for each element of `fileids`.\n",
    "\n",
    "Note, categories in the Reuters corpus overlap with each other, simply because a news story often covers multiple topics. If a document has more than one category, we will return **only the first category** (in alphabetical order).\n",
    "\n",
    "Since we using only one category for each `fileid`, the result from `get_categories_from_fileids()` should have the same length as the `fileids` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "0c2841884f1f0f617e1a1e33927519c2",
     "grade": false,
     "grade_id": "get_categories_from_fileids_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trade', 'grain', 'crude', 'corn', 'palm-oil'] ... ['interest', 'earn', 'earn', 'earn', 'earn']\n"
     ]
    }
   ],
   "source": [
    "categories = get_categories_from_fileids(reuters, fileids)\n",
    "print(categories[:5], '...', categories[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test sets\n",
    "\n",
    "The Reuters data set has already been grouped into a training set and a test set. See `fileids`.\n",
    "\n",
    "To create a training set, we iterate through `fileids` and find all `fileids` that start with \"train\". `X_train` is a list of **raw data** strings, which we obtain by using the `raw()` method. `y_train`  is a list of categories. We repeat for all `fileids` that start with \"test\". In the end, `train_test_split()` will return a 4-tuple of `X_train`, `X_test`, `y_train`, and `y_test`, each of which is a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "6e4e6ef51f106bf0c9425b5d20342e09",
     "grade": false,
     "grade_id": "train_test_split_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reuters, fileids, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC (no pipeline, no stop words)\n",
    "\n",
    "We use `CountVectorizer` to create a document term matrix, and apply `LinearSVC` algorithm to classify which topic each news document belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "cb64bfe7073821eaef41aa1cc0c3c054",
     "grade": false,
     "grade_id": "cv_svc_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC prediction accuracy = 88.3%\n"
     ]
    }
   ],
   "source": [
    "cv1, svc1, y_pred1 = cv_svc(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score1 = accuracy_score(y_pred1, y_test)\n",
    "print(\"SVC prediction accuracy = {0:3.1f}%\".format(100.0 * score1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC (Pipeline, no stop words)\n",
    "\n",
    "- Now we'll build a pipeline curing `CountVectorizer` and `LinearSVC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "c71d9f23cc54e942fe863321de4e106f",
     "grade": false,
     "grade_id": "cv_svc_pipe_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC prediction accuracy = 88.3%\n"
     ]
    }
   ],
   "source": [
    "clf2, y_pred2 = cv_svc_pipe(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score2 = accuracy_score(y_pred2, y_test)\n",
    "print(\"SVC prediction accuracy = {0:3.1f}%\".format(100.0 * score2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC (Pipeline and stop words)\n",
    "\n",
    "- Finally, we'll add English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "e9179f18ce0969ea4393b0b9e35487c8",
     "grade": false,
     "grade_id": "cv_svc_pipe_sw_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC prediction accuracy = 87.9%\n"
     ]
    }
   ],
   "source": [
    "clf3, y_pred3 = cv_svc_pipe_sw(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score3 = accuracy_score(y_pred3, y_test)\n",
    "print(\"SVC prediction accuracy = {0:3.1f}%\".format(100.0 * score3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline of TF-IDF and SVM with stop words\n",
    "\n",
    "- Now, we can build a pipeline by using `TfidfVectorizer` and `LinearSVC`. We'll also use English stop words here.\n",
    "- Remember that TF-IDF represents how important a word is in a document or corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "f5ff4e2941491ae84bb545aa2ce37850",
     "grade": false,
     "grade_id": "tfidf_svc_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC prediction accuracy =  89.8%\n"
     ]
    }
   ],
   "source": [
    "clf4, y_pred4 = tfidf_svc(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score4 = accuracy_score(y_pred4, y_test)\n",
    "print(\"SVC prediction accuracy = {0:5.1f}%\".format(100.0 * score4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining\n",
    "\n",
    "We will continue to use the Reuters corpus, this time to perform text mining tasks, such as n-grams, stemming, and clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## n-grams\n",
    "\n",
    "- Uses unigrams, bigrams, and trigrams,\n",
    "- We build a pipeline using `TfidfVectorizer` and `LinearSVC`,\n",
    "- Uses English stop words,\n",
    "- We also impose a minimum feature term that requires a term to be present in at least three documents, and\n",
    "- Our maximum frequency is set to 75%, such that any term occurring in more than 75% of all documents will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "5a8baa2c71a9662191a7f1367db11e10",
     "grade": false,
     "grade_id": "ngram_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC prediction accuracy =  90.2%\n"
     ]
    }
   ],
   "source": [
    "clf1, y_pred1 = ngram(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score1 = accuracy_score(y_pred1, y_test)\n",
    "print(\"SVC prediction accuracy = {0:5.1f}%\".format(100.0 * score1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "- Uses the `tokenize` method in the following code cell to incorporate the Porter Stemmer into the classification pipeline,\n",
    "- Uses unigrams, bigrams, and trigrams\n",
    "- We build a pipeline by using `TfidfVectorizer` and `LinearSVC`,\n",
    "- Uses English stop words,\n",
    "- We impose a minimum feature term that requires a term to be present in at least two documents, and\n",
    "- Our maximum frequency is set to 50%, such that any term occurring in more than 50% of all documents will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "746539c1cd15a09835d820327c2cbc48",
     "grade": false,
     "grade_id": "stem_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC prediction accuracy =  89.2%\n"
     ]
    }
   ],
   "source": [
    "clf2, y_pred2 = stem(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score2 = accuracy_score(y_pred2, y_test)\n",
    "print(\"SVC prediction accuracy = {0:5.1f}%\".format(100.0 * score2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clustering Analysis\n",
    "\n",
    "- We build a pipeline by using `TfidfVectorizer` and `KMeans`,\n",
    "- Uses only unigrams,\n",
    "- Uses English stop words, and\n",
    "- The number of clusters will be set to the value of `true_k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "8334a6f305b8ec5e81b7ba02b8344ced",
     "grade": false,
     "grade_id": "cluster_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "clf3 = cluster(X_train, X_test, true_k=len(reuters.categories()), random_state=check_random_state(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function `get_top_tokens()` identifies the most frequently used words in a cluster.\n",
    "\n",
    "The `cluster` parameter specifies the cluster label. Since we are not using training labels, the cluster labels returned by `KMeans` will be integers.\n",
    "\n",
    "If we were to set `top_tokens=3`, the function will return a list of top 3 tokens in the specified cluster; similarly, if we set `top_tokens=5`, it will return a list of top 5 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "a724f4b47ffb0aa9096cc1823df39f27",
     "grade": false,
     "grade_id": "get_top_tokens_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: units housing starts pct seasonally\n",
      "Cluster 1: vs cts shr net qtr\n",
      "Cluster 2: yeutter trade said clayton representative\n",
      "Cluster 3: steel nippon said yen year\n",
      "Cluster 4: shares offer said common stock\n",
      "Cluster 5: pct december billion january money\n",
      "Cluster 6: vs mln net cts shr\n",
      "Cluster 7: opec oil prices saudi bpd\n",
      "Cluster 8: usair twa piedmont application airlines\n",
      "Cluster 9: cts quarterly qtly div pay\n",
      "Cluster 10: oil said herrington tax energy\n",
      "Cluster 11: loss vs mln 000 cts\n",
      "Cluster 12: wheat tonnes 000 barley export\n",
      "Cluster 13: deficit mln account billion trade\n",
      "Cluster 14: canadian canada rapeseed said crushers\n",
      "Cluster 15: futures bp exchange oil said\n",
      "Cluster 16: stg bills mln money drain\n",
      "Cluster 17: cocoa buffer icco delegates stock\n",
      "Cluster 18: 50 crude bbl wti raises\n",
      "Cluster 19: taiwan billion trade foreign dlrs\n",
      "Cluster 20: yen dealers dollars bank japan\n",
      "Cluster 21: corn tonnes usda acres bushels\n",
      "Cluster 22: oper vs 000 net cts\n",
      "Cluster 23: stake shares pct group investment\n",
      "Cluster 24: gold ounces ton ounce said\n",
      "Cluster 25: split stock dividend record said\n",
      "Cluster 26: sumita japan bank nations dollar\n",
      "Cluster 27: pct february january index rose\n",
      "Cluster 28: rate pct rates bank base\n",
      "Cluster 29: american sale said savings lt\n",
      "Cluster 30: tonnes mln 000 1986 production\n",
      "Cluster 31: vs profit loss cts net\n",
      "Cluster 32: china weather said grain agency\n",
      "Cluster 33: mln vs profit stg tax\n",
      "Cluster 34: tonnes 000 palm pakistan shipment\n",
      "Cluster 35: cable viacom telecom said amusements\n",
      "Cluster 36: bank said banks debt foreign\n",
      "Cluster 37: quarter dlrs earnings share said\n",
      "Cluster 38: fed customer repurchase reserves federal\n",
      "Cluster 39: sugar tonnes traders white ec\n",
      "Cluster 40: coffee ico quotas export brazil\n",
      "Cluster 41: bpd mln crude oil eia\n",
      "Cluster 42: caesars sosnoff harper supermarkets iv\n",
      "Cluster 43: south africa african rand said\n",
      "Cluster 44: chrysler amc motors renault american\n",
      "Cluster 45: gulf iran oil iranian said\n",
      "Cluster 46: cyclops dixons cyacq offer tender\n",
      "Cluster 47: mar 1987 26 feb 20\n",
      "Cluster 48: stocks barrels distillate mln crude\n",
      "Cluster 49: said 000 grain apr usda\n",
      "Cluster 50: taft broadcasting dudley bass lindner\n",
      "Cluster 51: dollar bank said yen central\n",
      "Cluster 52: miyazawa yen dollar nations paris\n",
      "Cluster 53: marks bundesbank billion german pct\n",
      "Cluster 54: japan japanese trade said semiconductor\n",
      "Cluster 55: wheat soviet said lyng 92\n",
      "Cluster 56: purolator hutton emery courier offer\n",
      "Cluster 57: crowns billion ab vs mln\n",
      "Cluster 58: ccc credit department bonus export\n",
      "Cluster 59: rospatch diagnostic retrieval bid systems\n",
      "Cluster 60: pct growth year said rise\n",
      "Cluster 61: prime rate pct bank effective\n",
      "Cluster 62: fed week says dlrs loans\n",
      "Cluster 63: gencorp general offer partners afg\n",
      "Cluster 64: mln dlrs said 1986 company\n",
      "Cluster 65: indonesia palm oil tonnes indonesian\n",
      "Cluster 66: 000 vs profit loss cts\n",
      "Cluster 67: shell oil said exploration lt\n",
      "Cluster 68: billion dlrs surplus february january\n",
      "Cluster 69: said lt corp company acquisition\n",
      "Cluster 70: rubber tin pact inra price\n",
      "Cluster 71: baker korea said countries south\n",
      "Cluster 72: ec tax oils european community\n",
      "Cluster 73: refinery pdvsa champlin said venezuela\n",
      "Cluster 74: franklin march mthly fund cts\n",
      "Cluster 75: trade gatt said countries house\n",
      "Cluster 76: stg bank money market mln\n",
      "Cluster 77: baker treasury secretary says james\n",
      "Cluster 78: dome petroleum dlrs encor mln\n",
      "Cluster 79: cts payout div prior pay\n",
      "Cluster 80: oper loss vs 000 cts\n",
      "Cluster 81: vs 000 cts net mln\n",
      "Cluster 82: merger hughes said baker shareholders\n",
      "Cluster 83: ecuador oil pipeline venezuela crude\n",
      "Cluster 84: francs billion mln swiss vs\n",
      "Cluster 85: dividend payable declared record april\n",
      "Cluster 86: chemlawn waste management waiting ships\n",
      "Cluster 87: strike seamen union port workers\n",
      "Cluster 88: gas oil cubic natural feet\n",
      "Cluster 89: qtly cts div dividend record\n"
     ]
    }
   ],
   "source": [
    "km = clf3.named_steps['km']\n",
    "tf = clf3.named_steps['tf']\n",
    "\n",
    "for i in range(len(reuters.categories())):\n",
    "    print(\"Cluster {0}: {1}\".format(i, ' '.join(get_top_tokens(km, tf, i, 5))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
